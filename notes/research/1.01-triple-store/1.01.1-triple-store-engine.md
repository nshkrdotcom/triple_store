# Building a High-Performance Triple Store Engine in Elixir/Erlang

A high-performance triple store for millions of triples with persistent storage, OWL DL reasoning, and full SPARQL 1.1 support is achievable on the BEAM, but requires strategic NIF usage for compute-intensive operations while leveraging Erlang's strengths for concurrency and fault tolerance. This report provides concrete architectural recommendations across storage, query processing, reasoning, and BEAM-specific implementation patterns.

## Storage architecture favors B+trees with dictionary encoding

For persistent storage targeting read-heavy SPARQL workloads with moderate updates, **LMDB-style B+trees outperform LSM-trees**. Jena's benchmarks comparing TDB2 (custom B+tree) against TDB3 (RocksDB) found the B+tree faster on *all aspects*—read, write, and space—once integration overhead was factored in. RocksDB's write-ahead advantages (1.5-2x raw throughput) are negated by compaction latency spikes and read amplification from SSTable merging.

The recommended storage stack combines CubDB (pure Elixir append-only B+tree) for single-node deployments or RocksDB via Rustler NIFs for production workloads requiring proven durability. CubDB offers ACID transactions, no size limits (unlike DETS's 2GB cap), and zero cross-compilation issues—ideal for embedded or Nerves deployments. For maximum performance, RocksDB with column families provides **11 separate tables** following Oxigraph's architecture: one `id2str` dictionary table plus nine quad-ordering indices.

**Dictionary encoding** is non-negotiable for performance. Map every URI, literal, and blank node to a **64-bit integer ID** with type tagging in the high bits. Oxigraph's approach—128-bit SipHash for URIs plus inline encoding for small strings and numeric types—eliminates separate forward lookups for hashed terms. For Elixir:

```elixir
defmodule TermId do
  @type_uri      0b0001
  @type_integer  0b0100
  
  # Encode URI as type-tagged sequence number
  def encode_uri(seq), do: (@type_uri <<< 60) ||| seq
  
  # Inline xsd:integer directly in 60 bits
  def encode_integer(n) when n >= 0 and n < (1 <<< 59), 
    do: (@type_integer <<< 60) ||| n
end
```

Inline encoding for `xsd:integer` (i64), `xsd:decimal` (fixed-point i128), and `xsd:dateTime` (timestamp + timezone offset) enables native comparisons and range queries without dictionary lookups—critical for FILTER evaluation.

## Three indices handle all triple patterns efficiently

The **SPO, POS, OSP** triple index configuration provides O(log n) access for all eight triple patterns with ~3x storage overhead—the sweet spot between hexastore's six indices and minimal coverage. This pattern-to-index mapping covers every bound/unbound combination:

| Pattern | Best Index | Operation |
|---------|-----------|-----------|
| SPO, SP?, S?? | SPO | Prefix scan |
| ?PO, ?P? | POS | Prefix scan |
| ??O, S?O | OSP | Prefix scan |

For named graph (quad) support, add **GSPO, GPOS, GOSP** indices—Oxigraph uses nine total indices for full quad coverage. Store indices as big-endian binary keys for natural range query ordering:

```elixir
def spo_key(s, p, o), do: <<s::64-big, p::64-big, o::64-big>>
```

ETS `ordered_set` tables with `{write_concurrency: true, read_concurrency: true}` provide in-memory indexing with **100x+ better multi-core throughput** than OTP 21 via the Contention Adapting (CA) tree. For persistence, pair ETS with CubDB write-through or use RocksDB column families directly.

## SPARQL evaluation requires Leapfrog Triejoin for complex BGPs

The standard join algorithms—index nested loop, hash join, merge join—remain essential for simple patterns, but **Leapfrog Triejoin** delivers worst-case optimal performance for complex BGPs with multiple joins. The algorithm processes queries *variable-by-variable* rather than join-by-join:

```elixir
defmodule LeapfrogJoin do
  # Given k sorted iterators, find next value present in ALL
  def seek_all(iterators, target) do
    # Each iterator seeks to target
    positioned = Enum.map(iterators, &Iterator.seek(&1, target))
    max_val = positioned |> Enum.map(&Iterator.current/1) |> Enum.max()
    
    if Enum.all?(positioned, &(Iterator.current(&1) == max_val)),
      do: {:ok, max_val},
      else: seek_all(positioned, max_val)  # Leapfrog to max
  end
end
```

This requires trie-structured indices (or B+trees with prefix iteration)—naturally provided by the SPO/POS/OSP organization. Research shows Leapfrog Triejoin achieves **significant improvements on complex BGP queries** versus traditional pairwise joins, particularly for star and snowflake patterns common in RDF.

**Query optimization** combines rule-based transforms (filter pushing, constant propagation) with cost-based join ordering using statistics. Maintain predicate cardinalities and per-pattern selectivity estimates:

```elixir
%Statistics{
  predicate: "foaf:knows",
  triple_count: 50_000,
  distinct_subjects: 10_000,
  distinct_objects: 8_000
}
```

Use dynamic programming (DPccp algorithm) for join order selection on queries with 4+ patterns, falling back to variable-counting heuristics for simpler cases. The optimizer should emit an iterator-based physical plan using Elixir's `Stream` for lazy evaluation.

## OWL 2 RL provides tractable reasoning via forward chaining

Full OWL DL requires tableau algorithms with NEXPTIME complexity—impractical for millions of triples. Instead, implement **OWL 2 RL** (Rule Language profile), which translates to Datalog rules with **polynomial data complexity**. This profile covers:

- Class hierarchies (rdfs:subClassOf transitivity)
- Property hierarchies and characteristics (transitive, symmetric, inverse)
- Domain/range constraints
- owl:sameAs/owl:differentFrom
- HasValue, intersection, some values from (with restrictions)

Forward chaining with **full materialization** delivers 100-1000x faster query performance by pre-computing all inferred triples. RDFox achieves **6.1 million triples/second reasoning rate** using parallel semi-naive evaluation:

```elixir
defmodule OWLRLReasoner do
  # Semi-naive: only derive from facts added in previous iteration
  def materialize(store, rules) do
    delta = apply_rules(store, store.facts, rules)
    
    if MapSet.size(delta) == 0 do
      store
    else
      store = add_facts(store, delta)
      materialize_delta(store, delta, rules)
    end
  end
  
  defp materialize_delta(store, delta, rules) do
    new_delta = apply_rules(store, delta, rules)
    # ... continue until fixpoint
  end
end
```

For **incremental updates**, implement the Backward/Forward (B/F) algorithm rather than naive rematerialization. On deletion:
1. **Backward**: Identify facts derived from deleted triples
2. **Forward**: Re-derive facts with alternative derivations
3. **Delete**: Remove only facts with no remaining justifications

This avoids the over-deletion problem of the simpler DRed algorithm. Store derivation metadata (justifications) for efficient retraction—approximately **40-60 bytes overhead per derived fact**.

Separate **TBox (schema) from ABox (instances)** in storage. Precompute the class/property hierarchy at ontology load time and store it in `persistent_term` for zero-copy access during reasoning. This separation enables:
- TBox classification once at load (seconds)
- ABox materialization incrementally (milliseconds for small updates)

## Process architecture leverages BEAM concurrency

Design around the **actor model** with clear process boundaries:

```
┌─────────────────────────────────────────────────────────┐
│                  TripleStore Supervisor                  │
├───────────────────┬─────────────────┬───────────────────┤
│  IndexManager     │ QuerySupervisor │ WriteCoordinator  │
│  (owns ETS tabs)  │ (Task.Supervisor)│ (serializes writes)│
├───────────────────┴─────────────────┴───────────────────┤
│                    ReasonerWorkers                       │
│            (GenServer pool for materialization)          │
└─────────────────────────────────────────────────────────┘
```

**IndexManager** owns ETS tables as a GenServer to prevent table deletion on worker crashes. **QuerySupervisor** spawns isolated Task processes for each query—short-lived processes avoid GC pressure on large result sets. **WriteCoordinator** serializes updates through a single GenServer, batching small writes for efficiency.

For bulk loading, use **Broadway** or **Flow** for backpressure-aware parallel ingestion:

```elixir
defmodule TripleIngestion do
  use Broadway
  
  def handle_batch(:index, messages, _, _) do
    triples = Enum.map(messages, & &1.data)
    IndexManager.bulk_insert(triples)  # Batched ETS/RocksDB write
    messages
  end
end
```

SPARQL UPDATE operations should be serialized through WriteCoordinator with **snapshot isolation**—readers see a consistent view while writes buffer in a transaction context, then commit atomically to all indices.

## NIFs provide value for parsing and compression, not query execution

**Use NIFs for:**
- **RDF parsing**: Turtle/N-Triples parsing is CPU-bound string processing—Rust implementations are 5-10x faster than pure Elixir
- **SPARQL parsing**: PEG grammars compile efficiently in Rust
- **Compression**: LZ4/Snappy for storage, zstd for archival—native implementations critical
- **Storage backends**: RocksDB/LMDB memory-mapped access

**Keep in pure Elixir:**
- **Query execution**: Must be preemptible; long-running NIFs block schedulers
- **SPARQL algebra optimization**: Benefits from pattern matching and immutability
- **Reasoning rule evaluation**: Erlang's actor model enables parallel materialization
- **Transaction coordination**: Leverages OTP fault tolerance

For NIFs, **Rustler** is recommended over Zigler for ecosystem maturity, panic catching (prevents BEAM crashes), and `rustler_precompiled` for zero-compile distribution. Use dirty CPU schedulers for operations exceeding 1ms:

```rust
#[rustler::nif(schedule = "DirtyCpu")]
fn parse_turtle(data: Binary) -> Vec<Triple> {
    // Rust parsing implementation
}
```

## Memory management for million-triple datasets

At ~40 bytes per triple (dictionary-encoded), 10 million triples require **~400MB** for raw storage plus ~1.2GB for three indices—comfortably within single-node memory. Key optimizations:

**Binary handling**: Store large literals (>64 bytes) as refc binaries shared between processes. Use sub-binaries from match contexts to avoid copying during parsing:

```elixir
def parse_triple(<<subject::binary-size(s_len), " ", rest::binary>>) do
  # subject is a zero-copy sub-binary
end
```

**Construct output with iolists**, not binary concatenation—single allocation at the end:

```elixir
def serialize_results(bindings) do
  bindings
  |> Enum.map(&format_binding/1)
  |> IO.iodata_to_binary()  # Single copy
end
```

**Atomics for counters**: Use `:atomics` for statistics (triple counts, selectivity estimates) with lock-free updates—no GenServer bottleneck for increment operations.


This architecture achieves **sub-second query latency** for typical SPARQL patterns on 10M triples while maintaining the BEAM's operational advantages: hot code reloading for rule updates, supervision trees for fault recovery, and transparent distribution for future horizontal scaling. The hybrid approach—NIFs for parsing/storage, pure Elixir for query coordination—balances performance with the operational simplicity that makes Erlang systems reliable in production.
